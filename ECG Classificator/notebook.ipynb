{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T18:53:27.979095Z",
     "start_time": "2024-09-11T18:53:27.920989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import recall_score, accuracy_score, precision_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:30.752901Z",
     "start_time": "2024-09-11T05:49:30.738898Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "normal=pd.read_csv('data/ptbdb_normal.csv',header=None)\n",
    "abnormal=pd.read_csv('data/ptbdb_abnormal.csv',header=None)\n",
    "combined_df=pd.concat([normal,abnormal])\n",
    "shuffled_df =combined_df.sample(frac=1)\n",
    "shuffled_df.to_csv('data/ptbdb_shuffled.csv',index=False,header=False)\n",
    "shuffled_df\n",
    "\"\"\""
   ],
   "id": "2276f8ef20ec9c90",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnormal=pd.read_csv('data/ptbdb_normal.csv',header=None)\\nabnormal=pd.read_csv('data/ptbdb_abnormal.csv',header=None)\\ncombined_df=pd.concat([normal,abnormal])\\nshuffled_df =combined_df.sample(frac=1)\\nshuffled_df.to_csv('data/ptbdb_shuffled.csv',index=False,header=False)\\nshuffled_df\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:38.950325Z",
     "start_time": "2024-09-11T05:49:31.319845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "traindata = pd.read_csv('data/mitbih_train.csv',header=None)\n",
    "testdata = pd.read_csv('data/mitbih_test.csv',header=None)"
   ],
   "id": "123f420a3be91103",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:39.057999Z",
     "start_time": "2024-09-11T05:49:38.999375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "traindata = traindata[traindata.iloc[:, -1] != 0.0]\n",
    "testdata = testdata[testdata.iloc[:, -1] != 0.0]"
   ],
   "id": "c9ad1084f1d01dfa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:39.150093Z",
     "start_time": "2024-09-11T05:49:39.107072Z"
    }
   },
   "cell_type": "code",
   "source": "traindata",
   "id": "d751332e0086869b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "72471  1.000000  0.666667  0.100457  0.036530  0.073059  0.050228  0.018265   \n",
       "72472  0.983696  1.000000  0.331522  0.000000  0.108696  0.163043  0.130435   \n",
       "72473  1.000000  0.911504  0.216814  0.000000  0.101770  0.199115  0.176991   \n",
       "72474  0.090498  0.126697  0.217195  0.361991  0.461538  0.556561  0.443439   \n",
       "72475  0.961111  1.000000  0.551852  0.101852  0.040741  0.085185  0.094444   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "87549  0.807018  0.494737  0.536842  0.529825  0.491228  0.484211  0.456140   \n",
       "87550  0.718333  0.605000  0.486667  0.361667  0.231667  0.120000  0.051667   \n",
       "87551  0.906122  0.624490  0.595918  0.575510  0.530612  0.481633  0.444898   \n",
       "87552  0.858228  0.645570  0.845570  0.248101  0.167089  0.131646  0.121519   \n",
       "87553  0.901506  0.845886  0.800695  0.748552  0.687138  0.599073  0.512167   \n",
       "\n",
       "            7         8         9    ...       178       179       180  \\\n",
       "72471  0.105023  0.132420  0.091324  ...  0.000000  0.000000  0.000000   \n",
       "72472  0.190217  0.288043  0.222826  ...  0.461957  0.483696  0.500000   \n",
       "72473  0.194690  0.252212  0.238938  ...  0.000000  0.000000  0.000000   \n",
       "72474  0.434389  0.452489  0.511312  ...  0.122172  0.131222  0.140271   \n",
       "72475  0.088889  0.085185  0.070370  ...  0.000000  0.000000  0.000000   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "87549  0.396491  0.284211  0.136842  ...  0.000000  0.000000  0.000000   \n",
       "87550  0.001667  0.000000  0.013333  ...  0.000000  0.000000  0.000000   \n",
       "87551  0.387755  0.322449  0.191837  ...  0.000000  0.000000  0.000000   \n",
       "87552  0.121519  0.118987  0.103797  ...  0.000000  0.000000  0.000000   \n",
       "87553  0.427578  0.395133  0.402086  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "            181       182      183       184       185       186  187  \n",
       "72471  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  1.0  \n",
       "72472  0.494565  0.510870  0.51087  0.505435  0.472826  0.434783  1.0  \n",
       "72473  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  1.0  \n",
       "72474  0.158371  0.176471  0.20362  0.212670  0.000000  0.000000  1.0  \n",
       "72475  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  1.0  \n",
       "...         ...       ...      ...       ...       ...       ...  ...  \n",
       "87549  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  4.0  \n",
       "87550  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  4.0  \n",
       "87551  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  4.0  \n",
       "87552  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  4.0  \n",
       "87553  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  4.0  \n",
       "\n",
       "[15083 rows x 188 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72471</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.100457</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.073059</td>\n",
       "      <td>0.050228</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>0.105023</td>\n",
       "      <td>0.132420</td>\n",
       "      <td>0.091324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72472</th>\n",
       "      <td>0.983696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.190217</td>\n",
       "      <td>0.288043</td>\n",
       "      <td>0.222826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461957</td>\n",
       "      <td>0.483696</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.494565</td>\n",
       "      <td>0.510870</td>\n",
       "      <td>0.51087</td>\n",
       "      <td>0.505435</td>\n",
       "      <td>0.472826</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72473</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.216814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101770</td>\n",
       "      <td>0.199115</td>\n",
       "      <td>0.176991</td>\n",
       "      <td>0.194690</td>\n",
       "      <td>0.252212</td>\n",
       "      <td>0.238938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72474</th>\n",
       "      <td>0.090498</td>\n",
       "      <td>0.126697</td>\n",
       "      <td>0.217195</td>\n",
       "      <td>0.361991</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.556561</td>\n",
       "      <td>0.443439</td>\n",
       "      <td>0.434389</td>\n",
       "      <td>0.452489</td>\n",
       "      <td>0.511312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122172</td>\n",
       "      <td>0.131222</td>\n",
       "      <td>0.140271</td>\n",
       "      <td>0.158371</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.20362</td>\n",
       "      <td>0.212670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72475</th>\n",
       "      <td>0.961111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551852</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>0.085185</td>\n",
       "      <td>0.094444</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.085185</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87549</th>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.536842</td>\n",
       "      <td>0.529825</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.396491</td>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87550</th>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.486667</td>\n",
       "      <td>0.361667</td>\n",
       "      <td>0.231667</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87551</th>\n",
       "      <td>0.906122</td>\n",
       "      <td>0.624490</td>\n",
       "      <td>0.595918</td>\n",
       "      <td>0.575510</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.481633</td>\n",
       "      <td>0.444898</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.322449</td>\n",
       "      <td>0.191837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87552</th>\n",
       "      <td>0.858228</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.845570</td>\n",
       "      <td>0.248101</td>\n",
       "      <td>0.167089</td>\n",
       "      <td>0.131646</td>\n",
       "      <td>0.121519</td>\n",
       "      <td>0.121519</td>\n",
       "      <td>0.118987</td>\n",
       "      <td>0.103797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87553</th>\n",
       "      <td>0.901506</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>0.800695</td>\n",
       "      <td>0.748552</td>\n",
       "      <td>0.687138</td>\n",
       "      <td>0.599073</td>\n",
       "      <td>0.512167</td>\n",
       "      <td>0.427578</td>\n",
       "      <td>0.395133</td>\n",
       "      <td>0.402086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15083 rows × 188 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:39.364848Z",
     "start_time": "2024-09-11T05:49:39.321916Z"
    }
   },
   "cell_type": "code",
   "source": "testdata",
   "id": "f9446db460fe51e7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "18118  1.000000  0.758982  0.473054  0.193114  0.142216  0.176647  0.161677   \n",
       "18119  1.000000  0.746556  0.088154  0.035813  0.212121  0.253444  0.269972   \n",
       "18120  0.972840  0.728395  0.000000  0.037037  0.162963  0.143210  0.123457   \n",
       "18121  1.000000  0.694444  0.238095  0.321429  0.337302  0.337302  0.301587   \n",
       "18122  0.939815  0.791667  0.199074  0.000000  0.055556  0.101852  0.106481   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21887  0.928736  0.871264  0.804598  0.742529  0.650575  0.535632  0.394253   \n",
       "21888  0.802691  0.692078  0.587444  0.446936  0.318386  0.189836  0.118087   \n",
       "21889  1.000000  0.967359  0.620178  0.347181  0.139466  0.089021  0.103858   \n",
       "21890  0.984127  0.567460  0.607143  0.583333  0.607143  0.575397  0.575397   \n",
       "21891  0.973970  0.913232  0.865510  0.823210  0.746204  0.642082  0.547722   \n",
       "\n",
       "            7         8         9    ...  178  179  180  181  182  183  184  \\\n",
       "18118  0.142216  0.121257  0.107784  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "18119  0.283747  0.275482  0.272727  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "18120  0.140741  0.133333  0.096296  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "18121  0.325397  0.325397  0.333333  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "18122  0.143519  0.222222  0.189815  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "21887  0.250575  0.140230  0.102299  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21888  0.077728  0.112108  0.152466  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21889  0.100890  0.106825  0.100890  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21890  0.488095  0.392857  0.238095  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21891  0.426247  0.325380  0.279826  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       185  186  187  \n",
       "18118  0.0  0.0  1.0  \n",
       "18119  0.0  0.0  1.0  \n",
       "18120  0.0  0.0  1.0  \n",
       "18121  0.0  0.0  1.0  \n",
       "18122  0.0  0.0  1.0  \n",
       "...    ...  ...  ...  \n",
       "21887  0.0  0.0  4.0  \n",
       "21888  0.0  0.0  4.0  \n",
       "21889  0.0  0.0  4.0  \n",
       "21890  0.0  0.0  4.0  \n",
       "21891  0.0  0.0  4.0  \n",
       "\n",
       "[3774 rows x 188 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18118</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758982</td>\n",
       "      <td>0.473054</td>\n",
       "      <td>0.193114</td>\n",
       "      <td>0.142216</td>\n",
       "      <td>0.176647</td>\n",
       "      <td>0.161677</td>\n",
       "      <td>0.142216</td>\n",
       "      <td>0.121257</td>\n",
       "      <td>0.107784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18119</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.746556</td>\n",
       "      <td>0.088154</td>\n",
       "      <td>0.035813</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.253444</td>\n",
       "      <td>0.269972</td>\n",
       "      <td>0.283747</td>\n",
       "      <td>0.275482</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18120</th>\n",
       "      <td>0.972840</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.162963</td>\n",
       "      <td>0.143210</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.140741</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18121</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.337302</td>\n",
       "      <td>0.337302</td>\n",
       "      <td>0.301587</td>\n",
       "      <td>0.325397</td>\n",
       "      <td>0.325397</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18122</th>\n",
       "      <td>0.939815</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.199074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.106481</td>\n",
       "      <td>0.143519</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.189815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21887</th>\n",
       "      <td>0.928736</td>\n",
       "      <td>0.871264</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.742529</td>\n",
       "      <td>0.650575</td>\n",
       "      <td>0.535632</td>\n",
       "      <td>0.394253</td>\n",
       "      <td>0.250575</td>\n",
       "      <td>0.140230</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>0.802691</td>\n",
       "      <td>0.692078</td>\n",
       "      <td>0.587444</td>\n",
       "      <td>0.446936</td>\n",
       "      <td>0.318386</td>\n",
       "      <td>0.189836</td>\n",
       "      <td>0.118087</td>\n",
       "      <td>0.077728</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>0.152466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967359</td>\n",
       "      <td>0.620178</td>\n",
       "      <td>0.347181</td>\n",
       "      <td>0.139466</td>\n",
       "      <td>0.089021</td>\n",
       "      <td>0.103858</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.106825</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.567460</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.575397</td>\n",
       "      <td>0.575397</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21891</th>\n",
       "      <td>0.973970</td>\n",
       "      <td>0.913232</td>\n",
       "      <td>0.865510</td>\n",
       "      <td>0.823210</td>\n",
       "      <td>0.746204</td>\n",
       "      <td>0.642082</td>\n",
       "      <td>0.547722</td>\n",
       "      <td>0.426247</td>\n",
       "      <td>0.325380</td>\n",
       "      <td>0.279826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3774 rows × 188 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:40.759588Z",
     "start_time": "2024-09-11T05:49:40.734871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_test=testdata[len(testdata.columns)-1] #Taking the last column as y\n",
    "testdataNoY=testdata.drop(testdata.columns[[len(testdata.columns)-1]], axis=1) #Remove the last column(removing y)\n",
    "x_test = testdataNoY "
   ],
   "id": "2a4c514879fb14ff",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:41.051322Z",
     "start_time": "2024-09-11T05:49:40.992927Z"
    }
   },
   "cell_type": "code",
   "source": "x_test",
   "id": "84708278c3b3969",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "18118  1.000000  0.758982  0.473054  0.193114  0.142216  0.176647  0.161677   \n",
       "18119  1.000000  0.746556  0.088154  0.035813  0.212121  0.253444  0.269972   \n",
       "18120  0.972840  0.728395  0.000000  0.037037  0.162963  0.143210  0.123457   \n",
       "18121  1.000000  0.694444  0.238095  0.321429  0.337302  0.337302  0.301587   \n",
       "18122  0.939815  0.791667  0.199074  0.000000  0.055556  0.101852  0.106481   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21887  0.928736  0.871264  0.804598  0.742529  0.650575  0.535632  0.394253   \n",
       "21888  0.802691  0.692078  0.587444  0.446936  0.318386  0.189836  0.118087   \n",
       "21889  1.000000  0.967359  0.620178  0.347181  0.139466  0.089021  0.103858   \n",
       "21890  0.984127  0.567460  0.607143  0.583333  0.607143  0.575397  0.575397   \n",
       "21891  0.973970  0.913232  0.865510  0.823210  0.746204  0.642082  0.547722   \n",
       "\n",
       "            7         8         9    ...  177  178  179  180  181  182  183  \\\n",
       "18118  0.142216  0.121257  0.107784  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "18119  0.283747  0.275482  0.272727  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "18120  0.140741  0.133333  0.096296  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "18121  0.325397  0.325397  0.333333  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "18122  0.143519  0.222222  0.189815  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "...         ...       ...       ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "21887  0.250575  0.140230  0.102299  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21888  0.077728  0.112108  0.152466  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21889  0.100890  0.106825  0.100890  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21890  0.488095  0.392857  0.238095  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "21891  0.426247  0.325380  0.279826  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "       184  185  186  \n",
       "18118  0.0  0.0  0.0  \n",
       "18119  0.0  0.0  0.0  \n",
       "18120  0.0  0.0  0.0  \n",
       "18121  0.0  0.0  0.0  \n",
       "18122  0.0  0.0  0.0  \n",
       "...    ...  ...  ...  \n",
       "21887  0.0  0.0  0.0  \n",
       "21888  0.0  0.0  0.0  \n",
       "21889  0.0  0.0  0.0  \n",
       "21890  0.0  0.0  0.0  \n",
       "21891  0.0  0.0  0.0  \n",
       "\n",
       "[3774 rows x 187 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18118</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.758982</td>\n",
       "      <td>0.473054</td>\n",
       "      <td>0.193114</td>\n",
       "      <td>0.142216</td>\n",
       "      <td>0.176647</td>\n",
       "      <td>0.161677</td>\n",
       "      <td>0.142216</td>\n",
       "      <td>0.121257</td>\n",
       "      <td>0.107784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18119</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.746556</td>\n",
       "      <td>0.088154</td>\n",
       "      <td>0.035813</td>\n",
       "      <td>0.212121</td>\n",
       "      <td>0.253444</td>\n",
       "      <td>0.269972</td>\n",
       "      <td>0.283747</td>\n",
       "      <td>0.275482</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18120</th>\n",
       "      <td>0.972840</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.162963</td>\n",
       "      <td>0.143210</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.140741</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18121</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.337302</td>\n",
       "      <td>0.337302</td>\n",
       "      <td>0.301587</td>\n",
       "      <td>0.325397</td>\n",
       "      <td>0.325397</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18122</th>\n",
       "      <td>0.939815</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.199074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.106481</td>\n",
       "      <td>0.143519</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.189815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21887</th>\n",
       "      <td>0.928736</td>\n",
       "      <td>0.871264</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.742529</td>\n",
       "      <td>0.650575</td>\n",
       "      <td>0.535632</td>\n",
       "      <td>0.394253</td>\n",
       "      <td>0.250575</td>\n",
       "      <td>0.140230</td>\n",
       "      <td>0.102299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21888</th>\n",
       "      <td>0.802691</td>\n",
       "      <td>0.692078</td>\n",
       "      <td>0.587444</td>\n",
       "      <td>0.446936</td>\n",
       "      <td>0.318386</td>\n",
       "      <td>0.189836</td>\n",
       "      <td>0.118087</td>\n",
       "      <td>0.077728</td>\n",
       "      <td>0.112108</td>\n",
       "      <td>0.152466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21889</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967359</td>\n",
       "      <td>0.620178</td>\n",
       "      <td>0.347181</td>\n",
       "      <td>0.139466</td>\n",
       "      <td>0.089021</td>\n",
       "      <td>0.103858</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>0.106825</td>\n",
       "      <td>0.100890</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21890</th>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.567460</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.575397</td>\n",
       "      <td>0.575397</td>\n",
       "      <td>0.488095</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21891</th>\n",
       "      <td>0.973970</td>\n",
       "      <td>0.913232</td>\n",
       "      <td>0.865510</td>\n",
       "      <td>0.823210</td>\n",
       "      <td>0.746204</td>\n",
       "      <td>0.642082</td>\n",
       "      <td>0.547722</td>\n",
       "      <td>0.426247</td>\n",
       "      <td>0.325380</td>\n",
       "      <td>0.279826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3774 rows × 187 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:41.420344Z",
     "start_time": "2024-09-11T05:49:41.408085Z"
    }
   },
   "cell_type": "code",
   "source": "y_test",
   "id": "11b3399461976d0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18118    1.0\n",
       "18119    1.0\n",
       "18120    1.0\n",
       "18121    1.0\n",
       "18122    1.0\n",
       "        ... \n",
       "21887    4.0\n",
       "21888    4.0\n",
       "21889    4.0\n",
       "21890    4.0\n",
       "21891    4.0\n",
       "Name: 187, Length: 3774, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:41.497112Z",
     "start_time": "2024-09-11T05:49:41.468620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_train=traindata[len(traindata.columns)-1]#Taking the last column as y\n",
    "trainDataNoY=traindata.drop(traindata.columns[[len(traindata.columns)-1]], axis=1) #Remove the last column(removing y)\n",
    "x_train = trainDataNoY"
   ],
   "id": "2719ca831f76d2a7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:41.728218Z",
     "start_time": "2024-09-11T05:49:41.685109Z"
    }
   },
   "cell_type": "code",
   "source": "x_train",
   "id": "f3d08fcddcb1142c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "72471  1.000000  0.666667  0.100457  0.036530  0.073059  0.050228  0.018265   \n",
       "72472  0.983696  1.000000  0.331522  0.000000  0.108696  0.163043  0.130435   \n",
       "72473  1.000000  0.911504  0.216814  0.000000  0.101770  0.199115  0.176991   \n",
       "72474  0.090498  0.126697  0.217195  0.361991  0.461538  0.556561  0.443439   \n",
       "72475  0.961111  1.000000  0.551852  0.101852  0.040741  0.085185  0.094444   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "87549  0.807018  0.494737  0.536842  0.529825  0.491228  0.484211  0.456140   \n",
       "87550  0.718333  0.605000  0.486667  0.361667  0.231667  0.120000  0.051667   \n",
       "87551  0.906122  0.624490  0.595918  0.575510  0.530612  0.481633  0.444898   \n",
       "87552  0.858228  0.645570  0.845570  0.248101  0.167089  0.131646  0.121519   \n",
       "87553  0.901506  0.845886  0.800695  0.748552  0.687138  0.599073  0.512167   \n",
       "\n",
       "            7         8         9    ...       177       178       179  \\\n",
       "72471  0.105023  0.132420  0.091324  ...  0.000000  0.000000  0.000000   \n",
       "72472  0.190217  0.288043  0.222826  ...  0.483696  0.461957  0.483696   \n",
       "72473  0.194690  0.252212  0.238938  ...  0.000000  0.000000  0.000000   \n",
       "72474  0.434389  0.452489  0.511312  ...  0.113122  0.122172  0.131222   \n",
       "72475  0.088889  0.085185  0.070370  ...  0.000000  0.000000  0.000000   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "87549  0.396491  0.284211  0.136842  ...  0.000000  0.000000  0.000000   \n",
       "87550  0.001667  0.000000  0.013333  ...  0.000000  0.000000  0.000000   \n",
       "87551  0.387755  0.322449  0.191837  ...  0.000000  0.000000  0.000000   \n",
       "87552  0.121519  0.118987  0.103797  ...  0.000000  0.000000  0.000000   \n",
       "87553  0.427578  0.395133  0.402086  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "            180       181       182      183       184       185       186  \n",
       "72471  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "72472  0.500000  0.494565  0.510870  0.51087  0.505435  0.472826  0.434783  \n",
       "72473  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "72474  0.140271  0.158371  0.176471  0.20362  0.212670  0.000000  0.000000  \n",
       "72475  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "...         ...       ...       ...      ...       ...       ...       ...  \n",
       "87549  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "87550  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "87551  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "87552  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "87553  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[15083 rows x 187 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72471</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.100457</td>\n",
       "      <td>0.036530</td>\n",
       "      <td>0.073059</td>\n",
       "      <td>0.050228</td>\n",
       "      <td>0.018265</td>\n",
       "      <td>0.105023</td>\n",
       "      <td>0.132420</td>\n",
       "      <td>0.091324</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72472</th>\n",
       "      <td>0.983696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.331522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108696</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.190217</td>\n",
       "      <td>0.288043</td>\n",
       "      <td>0.222826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483696</td>\n",
       "      <td>0.461957</td>\n",
       "      <td>0.483696</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.494565</td>\n",
       "      <td>0.510870</td>\n",
       "      <td>0.51087</td>\n",
       "      <td>0.505435</td>\n",
       "      <td>0.472826</td>\n",
       "      <td>0.434783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72473</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.911504</td>\n",
       "      <td>0.216814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101770</td>\n",
       "      <td>0.199115</td>\n",
       "      <td>0.176991</td>\n",
       "      <td>0.194690</td>\n",
       "      <td>0.252212</td>\n",
       "      <td>0.238938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72474</th>\n",
       "      <td>0.090498</td>\n",
       "      <td>0.126697</td>\n",
       "      <td>0.217195</td>\n",
       "      <td>0.361991</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.556561</td>\n",
       "      <td>0.443439</td>\n",
       "      <td>0.434389</td>\n",
       "      <td>0.452489</td>\n",
       "      <td>0.511312</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113122</td>\n",
       "      <td>0.122172</td>\n",
       "      <td>0.131222</td>\n",
       "      <td>0.140271</td>\n",
       "      <td>0.158371</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.20362</td>\n",
       "      <td>0.212670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72475</th>\n",
       "      <td>0.961111</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.551852</td>\n",
       "      <td>0.101852</td>\n",
       "      <td>0.040741</td>\n",
       "      <td>0.085185</td>\n",
       "      <td>0.094444</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.085185</td>\n",
       "      <td>0.070370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87549</th>\n",
       "      <td>0.807018</td>\n",
       "      <td>0.494737</td>\n",
       "      <td>0.536842</td>\n",
       "      <td>0.529825</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.484211</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.396491</td>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87550</th>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.605000</td>\n",
       "      <td>0.486667</td>\n",
       "      <td>0.361667</td>\n",
       "      <td>0.231667</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.051667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87551</th>\n",
       "      <td>0.906122</td>\n",
       "      <td>0.624490</td>\n",
       "      <td>0.595918</td>\n",
       "      <td>0.575510</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.481633</td>\n",
       "      <td>0.444898</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.322449</td>\n",
       "      <td>0.191837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87552</th>\n",
       "      <td>0.858228</td>\n",
       "      <td>0.645570</td>\n",
       "      <td>0.845570</td>\n",
       "      <td>0.248101</td>\n",
       "      <td>0.167089</td>\n",
       "      <td>0.131646</td>\n",
       "      <td>0.121519</td>\n",
       "      <td>0.121519</td>\n",
       "      <td>0.118987</td>\n",
       "      <td>0.103797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87553</th>\n",
       "      <td>0.901506</td>\n",
       "      <td>0.845886</td>\n",
       "      <td>0.800695</td>\n",
       "      <td>0.748552</td>\n",
       "      <td>0.687138</td>\n",
       "      <td>0.599073</td>\n",
       "      <td>0.512167</td>\n",
       "      <td>0.427578</td>\n",
       "      <td>0.395133</td>\n",
       "      <td>0.402086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15083 rows × 187 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:42.066724Z",
     "start_time": "2024-09-11T05:49:42.053289Z"
    }
   },
   "cell_type": "code",
   "source": "y_train",
   "id": "619475157bca7511",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72471    1.0\n",
       "72472    1.0\n",
       "72473    1.0\n",
       "72474    1.0\n",
       "72475    1.0\n",
       "        ... \n",
       "87549    4.0\n",
       "87550    4.0\n",
       "87551    4.0\n",
       "87552    4.0\n",
       "87553    4.0\n",
       "Name: 187, Length: 15083, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>1. Data Preprocessing</h3>Data Cleaning: Check for missing or inconsistent data, and handle outliers if necessary.\n",
    "Data Encoding: Convert categorical variables into numerical ones using techniques like one-hot encoding or label encoding, if needed."
   ],
   "id": "9f84e3da3f8ddb8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>2. Sample balancing</h3>Class Imbalance Handling: Before moving forward, check if your dataset has class imbalance (e.g., significantly more samples of one class). If so, consider techniques like:\n",
    "\n",
    "Resampling: Either oversampling the minority class (e.g., SMOTE) or undersampling the majority class.\n",
    "Class Weights: Apply class weights in your model to handle imbalanced classes"
   ],
   "id": "d2bb4ea60c103da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h4>2.1 Analyzing the balance of the data</h4>",
   "id": "125b5a481dcd6e37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:42.343420Z",
     "start_time": "2024-09-11T05:49:42.331215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_frequencies=y_train.value_counts() #Count how many occurrences of each different classe there is in y \n",
    "print(\"Absolute Class Frequencies:\\n\", class_frequencies)"
   ],
   "id": "6c5f0d30566afeb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Class Frequencies:\n",
      " 187\n",
      "4.0    6431\n",
      "2.0    5788\n",
      "1.0    2223\n",
      "3.0     641\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:43.060137Z",
     "start_time": "2024-09-11T05:49:42.668606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fig=plt.figure(figsize=(10,5))\n",
    "class_frequencies.plot(kind='bar')\n",
    "plt.title('Class Frequencies')\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels if needed\n",
    "plt.show()"
   ],
   "id": "7736100b10bf7442",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:43.276542Z",
     "start_time": "2024-09-11T05:49:43.266228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class_proportions = y_train.value_counts(normalize=True)#Take the percentage of each different class in y\n",
    "print(\"Class Proportions (Relative Frequencies):\\n\", class_proportions)"
   ],
   "id": "78b7619332b3b5ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Proportions (Relative Frequencies):\n",
      " 187\n",
      "4.0    0.426374\n",
      "2.0    0.383743\n",
      "1.0    0.147384\n",
      "3.0    0.042498\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:43.553084Z",
     "start_time": "2024-09-11T05:49:43.387202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fig=plt.figure(figsize=(10,5))\n",
    "class_proportions.plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Class Proportions')\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ],
   "id": "2331c1c6a23dfe1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<strong>A common rule of thumb is that if the minority class represents less than 10–20% of the total samples, the data is often considered imbalanced.\n",
    "In our case the data is highly unbalanced , so we must apply a balancing data strategy.</strong> "
   ],
   "id": "984c7dd5de18e74a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h4>2.2 Applying balancing data strategy to the data</h4>",
   "id": "56d35c8405f719d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:43.645835Z",
     "start_time": "2024-09-11T05:49:43.633345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "print(\"----- Weighted loss function -----\")\n",
    "x = data[:,1:]\n",
    "y = data[:,0]\n",
    "\n",
    "clf = SVC(kernel = 'linear', class_weight='balanced')\n",
    "kf = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "cv_y_test = []\n",
    "cv_y_pred = []\n",
    "\n",
    "for train_index, test_index in kf.split(x, y):\n",
    "    \n",
    "    # Training phase\n",
    "    x_train = x[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Test phase\n",
    "    x_test = x[test_index, :]\n",
    "    y_test = y[test_index]    \n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    cv_y_test.append(y_test)\n",
    "    cv_y_pred.append(y_pred)\n",
    "\n",
    "print(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))\n",
    "\"\"\""
   ],
   "id": "a5b3f5346711ca6a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(\"----- Weighted loss function -----\")\\nx = data[:,1:]\\ny = data[:,0]\\n\\nclf = SVC(kernel = \\'linear\\', class_weight=\\'balanced\\')\\nkf = StratifiedKFold(n_splits=5, shuffle = True)\\n\\ncv_y_test = []\\ncv_y_pred = []\\n\\nfor train_index, test_index in kf.split(x, y):\\n    \\n    # Training phase\\n    x_train = x[train_index, :]\\n    y_train = y[train_index]\\n    clf.fit(x_train, y_train)\\n\\n    # Test phase\\n    x_test = x[test_index, :]\\n    y_test = y[test_index]    \\n    y_pred = clf.predict(x_test)\\n\\n    cv_y_test.append(y_test)\\n    cv_y_pred.append(y_pred)\\n\\nprint(classification_report(np.concatenate(cv_y_test), np.concatenate(cv_y_pred)))\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>3. Feature Selection</h3>Feature Importance Analysis: Use techniques such as correlation, mutual information, or methods like Recursive Feature Elimination (RFE) to select the most relevant features.",
   "id": "2c05d04a4a73a399"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>5. Train-Validate-Test: </h3>\n",
    "<ol>\n",
    "    <li>Dataset Splitting:\n",
    "        <ul>\n",
    "            <li>Divide the dataset into three sets: training, validation, and test.</li>\n",
    "            <li>Common splits are 70-15-15 or 80-10-10.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Model Training:\n",
    "        <ul>\n",
    "            <li>Train the model on the training set.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Validation:\n",
    "        <ul>\n",
    "            <li>Evaluate the model’s performance on the validation set.</li>\n",
    "            <li>Tweak hyperparameters based on validation results.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Final Evaluation:\n",
    "        <ul>\n",
    "            <li>Assess the model’s performance on the test set for a final unbiased evaluation.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ol>"
   ],
   "id": "9da7e8d7cc765bce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:43.754044Z",
     "start_time": "2024-09-11T05:49:43.740833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ModelResults:\n",
    "        def __init__(self,modelName=None, accuracy=None, recall=None, precision=None,selectedFeatures=None,featureRanking=None,report=None):\n",
    "            self.modelName = modelName\n",
    "            self.accuracy = accuracy\n",
    "            self.recall = recall\n",
    "            self.precision = precision\n",
    "            self.selectedFeatures = selectedFeatures\n",
    "            self.featureRanking = featureRanking\n",
    "            self.report = report\n",
    "        def __toString__(self):\n",
    "            return f\"ModelResults(model name={self.modelName}, accuracy={self.accuracy}, recall={self.recall}, precision={self.precision}, Selected Features={self.selectedFeatures},Feature Ranking={self.featureRanking}\\n{self.report})\""
   ],
   "id": "501ccce6d4874a76",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:43.862082Z",
     "start_time": "2024-09-11T05:49:43.835162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trainAndTestModel(modelName, model):\n",
    "    print(\"#######################################\")\n",
    "    print(modelName)\n",
    "    print(\"#######################################\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    model_accuracy = 0\n",
    "    model_recall = np.array([0., 0., 0., 0., 0.])\n",
    "    model_precision = np.array([0., 0., 0., 0., 0.])\n",
    "    selected_features,feature_ranking,report=0,0,0\n",
    "    \n",
    "    try:\n",
    "        # Crear RFE y Pipeline\n",
    "        rfe = RFE(model, n_features_to_select=54)\n",
    "        pipeline = Pipeline([\n",
    "            ('feature_selection', rfe),\n",
    "            ('classification', model)\n",
    "        ])\n",
    "        \n",
    "        # Entrenar el modelo con el pipeline\n",
    "        pipeline.fit(x_train, y_train)\n",
    "        \n",
    "        # Predecir con el modelo entrenado\n",
    "        y_pred = pipeline.predict(x_test)\n",
    "        \n",
    "        # Mostrar las características seleccionadas por RFE\n",
    "        selected_features = rfe.support_  # Boolean array de las features seleccionadas\n",
    "        feature_ranking = rfe.ranking_    # Ranking de todas las features\n",
    "        print(\"Características seleccionadas (True = seleccionada):\", selected_features)\n",
    "        print(\"Ranking de las características (1 = seleccionada):\", feature_ranking)\n",
    "        \n",
    "        # Evaluar el rendimiento del modelo    \n",
    "        report = classification_report(y_test, y_pred, zero_division=1)  # Aquí se añadió el parámetro zero_division\n",
    "        print(report)  # Imprimir el classification report\n",
    "        \n",
    "        # Calcular los scores de rendimiento\n",
    "        model_accuracy = accuracy_score(y_test, y_pred)\n",
    "        model_recall = recall_score(y_test, y_pred, average=None, zero_division=1)\n",
    "        model_precision = precision_score(y_test, y_pred, average=None, zero_division=1)\n",
    "        \n",
    "        print(f\"{modelName}, Done Training and testModel\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        return modelName, model_accuracy, model_recall, model_precision,selected_features,feature_ranking, report\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the train-test: {e}\")\n",
    "        return modelName, model_accuracy, model_recall, model_precision,selected_features,feature_ranking, report"
   ],
   "id": "19a72a243a64351b",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T05:49:43.969799Z",
     "start_time": "2024-09-11T05:49:43.957527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models = {\n",
    "    \"SVC\": SVC(kernel='linear', C=1.0, class_weight='balanced'),  # Handles class imbalance\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100, max_depth=5, class_weight='balanced'),  # Handles class imbalance\n",
    "    \"LogisticRegression\": LogisticRegression(C=1.0, solver='liblinear', class_weight='balanced'),  # Handles class imbalance\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(max_depth=4, class_weight='balanced'),  # Handles class imbalance\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(n_estimators=50, learning_rate=1.0), # Can be used with a base estimator that handles class imbalance\n",
    "}"
   ],
   "id": "473ddc69c16971cc",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T07:53:16.976201Z",
     "start_time": "2024-09-11T05:49:44.065295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "models_results=[]\n",
    "for name,model in models.items():\n",
    "    model_name,model_accuracy, model_recall, model_precision,selected_features,feature_ranking,report=trainAndTestModel(modelName=name,model=model)\n",
    "    models_results.append(ModelResults(model_name,model_accuracy, model_recall, model_precision,selected_features,feature_ranking,report))"
   ],
   "id": "2c271a34a4eb1c5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "SVC\n",
      "#######################################\n",
      "\n",
      "\n",
      "Características seleccionadas (True = seleccionada): [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True False  True  True  True  True  True False False\n",
      " False  True False False  True  True False  True  True  True  True False\n",
      " False False False  True False  True False False  True False False False\n",
      " False False False False False  True False False  True False False False\n",
      "  True False False False False False  True False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      "  True False False False False  True False False False  True False  True\n",
      " False False False False  True False False False False False  True False\n",
      " False False False False  True False False False False False  True False\n",
      " False False False False  True False False False False  True False False\n",
      " False False  True  True False False  True False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False  True  True False  True False\n",
      " False  True False False False False False]\n",
      "Ranking de las características (1 = seleccionada): [  1   1   1   1   1   1   1   1   1   1   1   1   1   1  20   1  45   1\n",
      "   1   1   1   1 122  85  53   1  24  59   1   1  98   1   1   1   1  31\n",
      "  62  26  25   1  16   1  38  15   1  69  10 133  52  74  47  84  58   1\n",
      " 116  71   1  44  95  27   1  11  57 106  32 105   1  56 104  30 103  64\n",
      " 118  94   1 129  70 117 125  37  36  60 121  61  40 101 114 115  93  39\n",
      "  96  35   1  55  17 100   1  75  76  50  51   1  67   3  49   1  43   1\n",
      "  66 120 132 109   1  34 127  92 119  73   1  88 128  48  12  65   1 111\n",
      "  89  79  42  99   1  78  14  72  82  81   1 107 110 124  46   1  86  54\n",
      " 126 113   1   1  23   9   1   2  90 131  91  41   5   6 123  28  29  83\n",
      "  18  68   1  87 112 130  80 134 102  63   8  77  19   1   1  13   1   4\n",
      "  22   1   7  33  21  97 108]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.86      0.85       556\n",
      "         2.0       0.91      0.86      0.88      1448\n",
      "         3.0       0.47      0.91      0.62       162\n",
      "         4.0       0.98      0.93      0.96      1608\n",
      "\n",
      "    accuracy                           0.89      3774\n",
      "   macro avg       0.80      0.89      0.83      3774\n",
      "weighted avg       0.91      0.89      0.90      3774\n",
      "\n",
      "SVC, Done Training and testModel\n",
      "\n",
      "\n",
      "#######################################\n",
      "RandomForestClassifier\n",
      "#######################################\n",
      "\n",
      "\n",
      "Características seleccionadas (True = seleccionada): [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False False False  True  True  True False False False\n",
      " False False False False False False False False  True False  True  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True  True  True  True False  True False False  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False]\n",
      "Ranking de las características (1 = seleccionada): [  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1  23  49  55  46  53   9\n",
      "  20  18  27  16  36   5   1   1   1   3  13  12  22  35  32  29  28  41\n",
      "  34   8   1   7   1   1   1  26  37  21  19  17  43  60  63  73  69  68\n",
      "  62  45  74  42  31  76  58  59  38  52  51  44  15   6   1   1   1   1\n",
      "   1  65   1   2  25   1   1   1   1   1   1   1   1   1  11  30  33  80\n",
      "  67  47 110  90  87  71 109  86  83  81 102  96  84 103  98  78  91  85\n",
      "  64  50  54  14   1   4  48  10   1  40  24  39  56  61  57  70  75  89\n",
      "  77  72  92  82  88  66  79 113  94  99 117 123  95 111 130  93 101 105\n",
      " 116  97 100 104 122 107 106 108 112 114 115 118 119 120 121 124 125 126\n",
      " 127 128 129 131 132 133 134]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.85      0.89      0.87       556\n",
      "         2.0       0.93      0.86      0.90      1448\n",
      "         3.0       0.55      0.91      0.69       162\n",
      "         4.0       0.97      0.95      0.96      1608\n",
      "\n",
      "    accuracy                           0.91      3774\n",
      "   macro avg       0.83      0.91      0.85      3774\n",
      "weighted avg       0.92      0.91      0.91      3774\n",
      "\n",
      "RandomForestClassifier, Done Training and testModel\n",
      "\n",
      "\n",
      "#######################################\n",
      "LogisticRegression\n",
      "#######################################\n",
      "\n",
      "\n",
      "Características seleccionadas (True = seleccionada): [ True  True  True  True  True  True False  True  True False  True  True\n",
      "  True  True  True  True False  True  True  True  True False False False\n",
      "  True  True  True False  True  True False False  True  True  True False\n",
      " False False False  True  True  True  True False False False  True False\n",
      " False False False False False  True False False  True False False False\n",
      "  True False False False  True False False False  True False False False\n",
      " False False  True False  True False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      "  True False False False False False False  True False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False False False False False  True False\n",
      " False  True False False  True False False False False  True False False\n",
      " False False  True  True False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False  True  True False False  True\n",
      " False False  True False False False False]\n",
      "Ranking de las características (1 = seleccionada): [  1   1   1   1   1   1  43   1   1   3   1   1   1   1   1   1  34   1\n",
      "   1   1   1   7  45  55   1   1   1  16   1   1  39   6   1   1   1  12\n",
      "  97  98  15   1   1   1   1  78  35  64   1 129  48 103   9 106  68   1\n",
      "  29  85   1  14  56  77   1  49 125  59   1  30  93  36   1  84 131 114\n",
      "  37  38   1  86   1 104  70  88  72 119  73 120  91  22 121 133  67 127\n",
      " 117  40   1  87  21  92   1  47 123   4  71  42  63   1  66  19 102   1\n",
      "  54 122 134 107 108  99  95 126  25 132 112  53 116 130  96  65   1 115\n",
      " 109  83  57  79   1 101  28   1  41  10   1  62   2  82  52   1  76 100\n",
      "  94  50   1   1 105  44  23  24  90 128  89  33   8  13 113  31  32  17\n",
      "   1  58  26  74 110 111 124  75 118  18  81  61  27   1   1  11  20   1\n",
      "  69  51   1  60  46  80   5]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.87      0.82      0.84       556\n",
      "         2.0       0.88      0.87      0.87      1448\n",
      "         3.0       0.47      0.74      0.58       162\n",
      "         4.0       0.96      0.94      0.95      1608\n",
      "\n",
      "    accuracy                           0.88      3774\n",
      "   macro avg       0.79      0.84      0.81      3774\n",
      "weighted avg       0.90      0.88      0.89      3774\n",
      "\n",
      "LogisticRegression, Done Training and testModel\n",
      "\n",
      "\n",
      "#######################################\n",
      "DecisionTreeClassifier\n",
      "#######################################\n",
      "\n",
      "\n",
      "Características seleccionadas (True = seleccionada): [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False]\n",
      "Ranking de las características (1 = seleccionada): [  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   2   5   6   9  11  13  15  17  19\n",
      "  21  23  25  27  29  31  33  35  37  39  41  43  45  47  49  51  53  55\n",
      "  57  59  61  62   1  67  69  71  73  75  77   1  81  83  85  87  89  91\n",
      "  93  95  97  99 101 103 105 107 109 111 113 115 117 119 121 123 125 127\n",
      "   1 131 133 134 132 130   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   3   4   7   8  10  12\n",
      "  14  16  18  20  22  24  26  28  30  32  34  36  38  40  42  44  46  48\n",
      "  50  52  54  56  58  60  63  64  65  66  68  70  72  74  76  78  79  80\n",
      "  82  84  86  88  90  92  94  96  98 100 102 104 106 108 110 112 114 116\n",
      " 118 120 122 124 126 128 129]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.71      0.87      0.78       556\n",
      "         2.0       0.85      0.70      0.77      1448\n",
      "         3.0       0.50      0.83      0.62       162\n",
      "         4.0       0.86      0.88      0.87      1608\n",
      "\n",
      "    accuracy                           0.80      3774\n",
      "   macro avg       0.73      0.82      0.76      3774\n",
      "weighted avg       0.82      0.80      0.81      3774\n",
      "\n",
      "DecisionTreeClassifier, Done Training and testModel\n",
      "\n",
      "\n",
      "#######################################\n",
      "AdaBoostClassifier\n",
      "#######################################\n",
      "\n",
      "\n",
      "Características seleccionadas (True = seleccionada): [ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True False False False False  True  True False False False\n",
      "  True False False False False  True False False False  True  True  True\n",
      " False  True  True  True False False False False False False  True  True\n",
      " False False False False False False False False  True False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False  True  True False False False False\n",
      "  True False False False False False False False  True  True False False\n",
      " False  True False False  True  True  True  True  True  True False  True\n",
      " False False False False False False  True  True False False False  True\n",
      "  True False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False]\n",
      "Ranking de las características (1 = seleccionada): [  1   1   1   1   1   1   1   1   1   1   1   1   1   5   1   2   6   8\n",
      "  10   1   1  22  21  17   1  26  28  30  32   1  35  37  39   1   1   1\n",
      "  11   1   1   1  57  59  61  62  60  58   1   1  43  45  47  49  51  53\n",
      "  55  56   1  63  65  67   1  71  73  75  77  79  81  83  85  87  89  91\n",
      "  93  95  97  99 101 103 105 107 109 111 113 115 117 119 121 123   1 127\n",
      "   1   1 133 134 132  54   1   9   7  13  14  16  18  19   1   1  25  27\n",
      "  23   1   4   3   1   1   1   1   1   1  20   1  29  31  33  24  15  12\n",
      "   1   1  34  36  38   1   1  42  44  46  48  50  52  40  41   1   1   1\n",
      "   1   1   1   1   1  64  66  68  69  70  72  74  76  78  80  82  84  86\n",
      "  88  90  92  94  96  98 100 102 104 106 108 110 112 114 116 118 120 122\n",
      " 124 125 126 128 129 130 131]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.85      0.83      0.84       556\n",
      "         2.0       0.83      0.89      0.86      1448\n",
      "         3.0       0.64      0.57      0.60       162\n",
      "         4.0       0.95      0.91      0.93      1608\n",
      "\n",
      "    accuracy                           0.88      3774\n",
      "   macro avg       0.82      0.80      0.81      3774\n",
      "weighted avg       0.88      0.88      0.88      3774\n",
      "\n",
      "AdaBoostClassifier, Done Training and testModel\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T07:53:17.098932Z",
     "start_time": "2024-09-11T07:53:17.073003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for results in models_results:\n",
    "    print(results.__toString__())"
   ],
   "id": "e3b735d95944b487",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResults(model name=SVC, accuracy=0.8905670376258612, recall=[0.85611511 0.86118785 0.90740741 0.92723881], precision=[0.83950617 0.90559187 0.46518987 0.98480845], Selected Features=[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True False  True False  True  True  True  True  True False False\n",
      " False  True False False  True  True False  True  True  True  True False\n",
      " False False False  True False  True False False  True False False False\n",
      " False False False False False  True False False  True False False False\n",
      "  True False False False False False  True False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      "  True False False False False  True False False False  True False  True\n",
      " False False False False  True False False False False False  True False\n",
      " False False False False  True False False False False False  True False\n",
      " False False False False  True False False False False  True False False\n",
      " False False  True  True False False  True False False False False False\n",
      " False False False False False False False False  True False False False\n",
      " False False False False False False False  True  True False  True False\n",
      " False  True False False False False False],Feature Ranking=[  1   1   1   1   1   1   1   1   1   1   1   1   1   1  20   1  45   1\n",
      "   1   1   1   1 122  85  53   1  24  59   1   1  98   1   1   1   1  31\n",
      "  62  26  25   1  16   1  38  15   1  69  10 133  52  74  47  84  58   1\n",
      " 116  71   1  44  95  27   1  11  57 106  32 105   1  56 104  30 103  64\n",
      " 118  94   1 129  70 117 125  37  36  60 121  61  40 101 114 115  93  39\n",
      "  96  35   1  55  17 100   1  75  76  50  51   1  67   3  49   1  43   1\n",
      "  66 120 132 109   1  34 127  92 119  73   1  88 128  48  12  65   1 111\n",
      "  89  79  42  99   1  78  14  72  82  81   1 107 110 124  46   1  86  54\n",
      " 126 113   1   1  23   9   1   2  90 131  91  41   5   6 123  28  29  83\n",
      "  18  68   1  87 112 130  80 134 102  63   8  77  19   1   1  13   1   4\n",
      "  22   1   7  33  21  97 108]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.84      0.86      0.85       556\n",
      "         2.0       0.91      0.86      0.88      1448\n",
      "         3.0       0.47      0.91      0.62       162\n",
      "         4.0       0.98      0.93      0.96      1608\n",
      "\n",
      "    accuracy                           0.89      3774\n",
      "   macro avg       0.80      0.89      0.83      3774\n",
      "weighted avg       0.91      0.89      0.90      3774\n",
      ")\n",
      "ModelResults(model name=RandomForestClassifier, accuracy=0.9075251722310546, recall=[0.89208633 0.86187845 0.91358025 0.95335821], precision=[0.85370052 0.93413174 0.55018587 0.96536524], Selected Features=[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      " False False False False False False  True  True  True False False False\n",
      " False False False False False False False False  True False  True  True\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True  True  True  True False  True False False  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False],Feature Ranking=[  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1  23  49  55  46  53   9\n",
      "  20  18  27  16  36   5   1   1   1   3  13  12  22  35  32  29  28  41\n",
      "  34   8   1   7   1   1   1  26  37  21  19  17  43  60  63  73  69  68\n",
      "  62  45  74  42  31  76  58  59  38  52  51  44  15   6   1   1   1   1\n",
      "   1  65   1   2  25   1   1   1   1   1   1   1   1   1  11  30  33  80\n",
      "  67  47 110  90  87  71 109  86  83  81 102  96  84 103  98  78  91  85\n",
      "  64  50  54  14   1   4  48  10   1  40  24  39  56  61  57  70  75  89\n",
      "  77  72  92  82  88  66  79 113  94  99 117 123  95 111 130  93 101 105\n",
      " 116  97 100 104 122 107 106 108 112 114 115 118 119 120 121 124 125 126\n",
      " 127 128 129 131 132 133 134]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.85      0.89      0.87       556\n",
      "         2.0       0.93      0.86      0.90      1448\n",
      "         3.0       0.55      0.91      0.69       162\n",
      "         4.0       0.97      0.95      0.96      1608\n",
      "\n",
      "    accuracy                           0.91      3774\n",
      "   macro avg       0.83      0.91      0.85      3774\n",
      "weighted avg       0.92      0.91      0.91      3774\n",
      ")\n",
      "ModelResults(model name=LogisticRegression, accuracy=0.8844727080021197, recall=[0.82194245 0.86740331 0.74074074 0.93594527], precision=[0.8655303  0.88202247 0.47058824 0.96043395], Selected Features=[ True  True  True  True  True  True False  True  True False  True  True\n",
      "  True  True  True  True False  True  True  True  True False False False\n",
      "  True  True  True False  True  True False False  True  True  True False\n",
      " False False False  True  True  True  True False False False  True False\n",
      " False False False False False  True False False  True False False False\n",
      "  True False False False  True False False False  True False False False\n",
      " False False  True False  True False False False False False False False\n",
      " False False False False False False False False  True False False False\n",
      "  True False False False False False False  True False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False False False False False  True False\n",
      " False  True False False  True False False False False  True False False\n",
      " False False  True  True False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False  True  True False False  True\n",
      " False False  True False False False False],Feature Ranking=[  1   1   1   1   1   1  43   1   1   3   1   1   1   1   1   1  34   1\n",
      "   1   1   1   7  45  55   1   1   1  16   1   1  39   6   1   1   1  12\n",
      "  97  98  15   1   1   1   1  78  35  64   1 129  48 103   9 106  68   1\n",
      "  29  85   1  14  56  77   1  49 125  59   1  30  93  36   1  84 131 114\n",
      "  37  38   1  86   1 104  70  88  72 119  73 120  91  22 121 133  67 127\n",
      " 117  40   1  87  21  92   1  47 123   4  71  42  63   1  66  19 102   1\n",
      "  54 122 134 107 108  99  95 126  25 132 112  53 116 130  96  65   1 115\n",
      " 109  83  57  79   1 101  28   1  41  10   1  62   2  82  52   1  76 100\n",
      "  94  50   1   1 105  44  23  24  90 128  89  33   8  13 113  31  32  17\n",
      "   1  58  26  74 110 111 124  75 118  18  81  61  27   1   1  11  20   1\n",
      "  69  51   1  60  46  80   5]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.87      0.82      0.84       556\n",
      "         2.0       0.88      0.87      0.87      1448\n",
      "         3.0       0.47      0.74      0.58       162\n",
      "         4.0       0.96      0.94      0.95      1608\n",
      "\n",
      "    accuracy                           0.88      3774\n",
      "   macro avg       0.79      0.84      0.81      3774\n",
      "weighted avg       0.90      0.88      0.89      3774\n",
      ")\n",
      "ModelResults(model name=DecisionTreeClassifier, accuracy=0.8049814520402756, recall=[0.86690647 0.69889503 0.83333333 0.87624378], precision=[0.7057101  0.85328836 0.49632353 0.86282915], Selected Features=[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False],Feature Ranking=[  1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   2   5   6   9  11  13  15  17  19\n",
      "  21  23  25  27  29  31  33  35  37  39  41  43  45  47  49  51  53  55\n",
      "  57  59  61  62   1  67  69  71  73  75  77   1  81  83  85  87  89  91\n",
      "  93  95  97  99 101 103 105 107 109 111 113 115 117 119 121 123 125 127\n",
      "   1 131 133 134 132 130   1   1   1   1   1   1   1   1   1   1   1   1\n",
      "   1   1   1   1   1   1   1   1   1   1   1   1   3   4   7   8  10  12\n",
      "  14  16  18  20  22  24  26  28  30  32  34  36  38  40  42  44  46  48\n",
      "  50  52  54  56  58  60  63  64  65  66  68  70  72  74  76  78  79  80\n",
      "  82  84  86  88  90  92  94  96  98 100 102 104 106 108 110 112 114 116\n",
      " 118 120 122 124 126 128 129]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.71      0.87      0.78       556\n",
      "         2.0       0.85      0.70      0.77      1448\n",
      "         3.0       0.50      0.83      0.62       162\n",
      "         4.0       0.86      0.88      0.87      1608\n",
      "\n",
      "    accuracy                           0.80      3774\n",
      "   macro avg       0.73      0.82      0.76      3774\n",
      "weighted avg       0.82      0.80      0.81      3774\n",
      ")\n",
      "ModelResults(model name=AdaBoostClassifier, accuracy=0.8757286698463169, recall=[0.83093525 0.89433702 0.56790123 0.90547264], precision=[0.85397412 0.8349452  0.63888889 0.94668401], Selected Features=[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True False  True False False False False  True  True False False False\n",
      "  True False False False False  True False False False  True  True  True\n",
      " False  True  True  True False False False False False False  True  True\n",
      " False False False False False False False False  True False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False  True False  True  True False False False False\n",
      "  True False False False False False False False  True  True False False\n",
      " False  True False False  True  True  True  True  True  True False  True\n",
      " False False False False False False  True  True False False False  True\n",
      "  True False False False False False False False False  True  True  True\n",
      "  True  True  True  True  True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False],Feature Ranking=[  1   1   1   1   1   1   1   1   1   1   1   1   1   5   1   2   6   8\n",
      "  10   1   1  22  21  17   1  26  28  30  32   1  35  37  39   1   1   1\n",
      "  11   1   1   1  57  59  61  62  60  58   1   1  43  45  47  49  51  53\n",
      "  55  56   1  63  65  67   1  71  73  75  77  79  81  83  85  87  89  91\n",
      "  93  95  97  99 101 103 105 107 109 111 113 115 117 119 121 123   1 127\n",
      "   1   1 133 134 132  54   1   9   7  13  14  16  18  19   1   1  25  27\n",
      "  23   1   4   3   1   1   1   1   1   1  20   1  29  31  33  24  15  12\n",
      "   1   1  34  36  38   1   1  42  44  46  48  50  52  40  41   1   1   1\n",
      "   1   1   1   1   1  64  66  68  69  70  72  74  76  78  80  82  84  86\n",
      "  88  90  92  94  96  98 100 102 104 106 108 110 112 114 116 118 120 122\n",
      " 124 125 126 128 129 130 131]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.85      0.83      0.84       556\n",
      "         2.0       0.83      0.89      0.86      1448\n",
      "         3.0       0.64      0.57      0.60       162\n",
      "         4.0       0.95      0.91      0.93      1608\n",
      "\n",
      "    accuracy                           0.88      3774\n",
      "   macro avg       0.82      0.80      0.81      3774\n",
      "weighted avg       0.88      0.88      0.88      3774\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>6. Model Selection and Hyperparameter Tuning</h3>Initial Model Training: Select a few candidate models based on your problem type (e.g., Logistic Regression, SVM, Random Forest).\n",
    "Hyperparameter Tuning: Use techniques like Grid Search or Random Search to find the best combination of hyperparameters for your model, ideally with cross-validation incorporated."
   ],
   "id": "b750a506569ec1a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T18:53:57.187200Z",
     "start_time": "2024-09-11T18:53:57.155850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trainAndTestModelWithHyperparameters(modelName, model, param_grid, x_train_selected, x_test_selected):\n",
    "    print(\"#######################################\")\n",
    "    print(modelName)\n",
    "    print(\"#######################################\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    model_accuracy = 0\n",
    "    model_recall = np.array([0., 0., 0., 0., 0.])\n",
    "    model_precision = np.array([0., 0., 0., 0., 0.])\n",
    "    report = 0\n",
    "    \n",
    "    try:\n",
    "        # Configurar la búsqueda aleatoria de hiperparámetros\n",
    "        random_search = RandomizedSearchCV(model, param_grid, n_iter=100, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
    "        \n",
    "        # Entrenar el modelo con los hiperparámetros optimizados\n",
    "        random_search.fit(x_train_selected, y_train)\n",
    "        \n",
    "        # Obtener los mejores hiperparámetros\n",
    "        best_params = random_search.best_params_\n",
    "        print(f\"Mejores hiperparámetros encontrados: {best_params}\")\n",
    "        \n",
    "        # Predecir con el modelo entrenado con los mejores hiperparámetros\n",
    "        y_pred = random_search.predict(x_test_selected)\n",
    "        \n",
    "        # Evaluar el rendimiento del modelo    \n",
    "        report = classification_report(y_test, y_pred, zero_division=1)\n",
    "        print(report)  # Imprimir el classification report\n",
    "        \n",
    "        # Calcular los scores de rendimiento\n",
    "        model_accuracy = accuracy_score(y_test, y_pred)\n",
    "        model_recall = recall_score(y_test, y_pred, average=None, zero_division=1)\n",
    "        model_precision = precision_score(y_test, y_pred, average=None, zero_division=1)\n",
    "        \n",
    "        print(f\"{modelName}, Done Training and Testing with Hyperparameters\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        return modelName, model_accuracy, model_recall, model_precision, best_params, report\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during the train-test: {e}\")\n",
    "        return modelName, model_accuracy, model_recall, model_precision, {}, report\n"
   ],
   "id": "2ed47333d2de020",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T19:10:21.645988Z",
     "start_time": "2024-09-11T19:10:21.601054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filtrar las características seleccionadas\n",
    "\n",
    "selected_features = np.array([ True,  True,  True,  True,  True,  True, False,  True,  True, False,  True,  True,\n",
    "  True,  True,  True,  True, False,  True,  True,  True,  True, False, False, False,\n",
    "  True,  True,  True, False,  True,  True, False, False,  True,  True,  True, False,\n",
    " False, False, False,  True,  True,  True,  True, False, False, False,  True, False,\n",
    " False, False, False, False, False,  True, False, False,  True, False, False, False,\n",
    "  True, False, False, False,  True, False, False, False,  True, False, False, False,\n",
    " False, False,  True, False,  True, False, False, False, False, False, False, False,\n",
    " False, False, False, False, False, False, False, False,  True, False, False, False,\n",
    "  True, False, False, False, False, False, False,  True, False, False, False,  True,\n",
    " False, False, False, False, False, False, False, False, False, False, False, False,\n",
    " False, False, False, False,  True, False, False, False, False, False,  True, False,\n",
    " False,  True, False, False,  True, False, False, False, False,  True, False, False,\n",
    " False, False,  True,  True, False, False, False, False, False, False, False, False,\n",
    " False, False, False, False, False, False,  True, False, False, False, False, False,\n",
    " False, False, False, False, False, False,  True,  True, False, False,  True,\n",
    " False, False,  True, False, False, False, False,False])\n",
    "\n",
    "print(f\"Shape of x_train: {x_train.shape}\")\n",
    "print(f\"Shape of x_test: {x_test.shape}\")\n",
    "print(f\"Length of selected_features: {len(selected_features)}\")\n"
   ],
   "id": "e657c22c0d00cec4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (15083, 187)\n",
      "Shape of x_test: (3774, 187)\n",
      "Length of selected_features: 187\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T19:10:26.822141Z",
     "start_time": "2024-09-11T19:10:26.774136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train_selected = x_train.iloc[:, selected_features]\n",
    "x_test_selected = x_test.iloc[:, selected_features]"
   ],
   "id": "8d5a5705a1b1926",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T19:22:28.561742400Z",
     "start_time": "2024-09-11T19:14:07.596109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hiperparámetros a probar en el modelo RandomForestClassifier\n",
    "param_grid = {\n",
    "   'n_estimators': [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [int(x) for x in np.linspace(10, 110, num=11)] + [None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "selected_model=RandomForestClassifier()\n",
    "\n",
    "# Llamada a la función con RandomForestClassifier\n",
    "trainAndTestModelWithHyperparameters('RandomForest', selected_model, param_grid, x_train_selected, x_test_selected)"
   ],
   "id": "b42a31c1183886f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################\n",
      "RandomForest\n",
      "#######################################\n",
      "\n",
      "\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[47], line 13\u001B[0m\n\u001B[0;32m     10\u001B[0m selected_model\u001B[38;5;241m=\u001B[39mRandomForestClassifier()\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# Llamada a la función con RandomForestClassifier\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[43mtrainAndTestModelWithHyperparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRandomForest\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mselected_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_grid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_train_selected\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_test_selected\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[26], line 17\u001B[0m, in \u001B[0;36mtrainAndTestModelWithHyperparameters\u001B[1;34m(modelName, model, param_grid, x_train_selected, x_test_selected)\u001B[0m\n\u001B[0;32m     14\u001B[0m random_search \u001B[38;5;241m=\u001B[39m RandomizedSearchCV(model, param_grid, n_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Entrenar el modelo con los hiperparámetros optimizados\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m \u001B[43mrandom_search\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_train_selected\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Obtener los mejores hiperparámetros\u001B[39;00m\n\u001B[0;32m     20\u001B[0m best_params \u001B[38;5;241m=\u001B[39m random_search\u001B[38;5;241m.\u001B[39mbest_params_\n",
      "File \u001B[1;32mC:\\Python\\Reto\\ECG-Heartbeat-Categorization\\ECG Classificator\\venv\\lib\\site-packages\\sklearn\\base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1150\u001B[0m     )\n\u001B[0;32m   1151\u001B[0m ):\n\u001B[1;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python\\Reto\\ECG-Heartbeat-Categorization\\ECG Classificator\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001B[0m, in \u001B[0;36mBaseSearchCV.fit\u001B[1;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[0;32m    892\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_results(\n\u001B[0;32m    893\u001B[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001B[0;32m    894\u001B[0m     )\n\u001B[0;32m    896\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m--> 898\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mevaluate_candidates\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    900\u001B[0m \u001B[38;5;66;03m# multimetric is determined here because in the case of a callable\u001B[39;00m\n\u001B[0;32m    901\u001B[0m \u001B[38;5;66;03m# self.scoring the return type is only known after calling\u001B[39;00m\n\u001B[0;32m    902\u001B[0m first_test_score \u001B[38;5;241m=\u001B[39m all_out[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_scores\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32mC:\\Python\\Reto\\ECG-Heartbeat-Categorization\\ECG Classificator\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1809\u001B[0m, in \u001B[0;36mRandomizedSearchCV._run_search\u001B[1;34m(self, evaluate_candidates)\u001B[0m\n\u001B[0;32m   1807\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_run_search\u001B[39m(\u001B[38;5;28mself\u001B[39m, evaluate_candidates):\n\u001B[0;32m   1808\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001B[39;00m\n\u001B[1;32m-> 1809\u001B[0m     \u001B[43mevaluate_candidates\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1810\u001B[0m \u001B[43m        \u001B[49m\u001B[43mParameterSampler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1811\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparam_distributions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrandom_state\u001B[49m\n\u001B[0;32m   1812\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1813\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python\\Reto\\ECG-Heartbeat-Categorization\\ECG Classificator\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001B[0m, in \u001B[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001B[1;34m(candidate_params, cv, more_results)\u001B[0m\n\u001B[0;32m    837\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    838\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    839\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFitting \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m folds for each of \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m candidates,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    840\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m totalling \u001B[39m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m fits\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    841\u001B[0m             n_splits, n_candidates, n_candidates \u001B[38;5;241m*\u001B[39m n_splits\n\u001B[0;32m    842\u001B[0m         )\n\u001B[0;32m    843\u001B[0m     )\n\u001B[1;32m--> 845\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    847\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_estimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    848\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    849\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    850\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    851\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    852\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    853\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_splits\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    854\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcandidate_progress\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_candidates\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    855\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_and_score_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    856\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcand_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparameters\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mproduct\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcandidate_params\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(out) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    863\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    864\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo fits were performed. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    865\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWas the CV iterator empty? \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    866\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWere there no candidates?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    867\u001B[0m     )\n",
      "File \u001B[1;32mC:\\Python\\Reto\\ECG-Heartbeat-Categorization\\ECG Classificator\\venv\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     60\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     61\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     62\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     64\u001B[0m )\n\u001B[1;32m---> 65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python\\Reto\\ECG-Heartbeat-Categorization\\ECG Classificator\\venv\\lib\\site-packages\\joblib\\parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[0;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Python\\Reto\\ECG-Heartbeat-Categorization\\ECG Classificator\\venv\\lib\\site-packages\\joblib\\parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Python\\Reto\\ECG-Heartbeat-Categorization\\ECG Classificator\\venv\\lib\\site-packages\\joblib\\parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[0;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[0;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[0;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<h3>7. Model Evaluation</h3>\n",
    "Evaluate on Test Data: After tuning, use the test set to evaluate your model using performance metrics like accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "ROC Curve and AUC for binary classification tasks."
   ],
   "id": "c7d0594f26e75c1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T07:53:17.223183Z",
     "start_time": "2024-09-11T07:53:17.209929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trainFinalModelWithRFE(modelName, model, x_total, y_total):\n",
    "    print(\"#######################################\")\n",
    "    print(f\"Training Final Model: {modelName}\")\n",
    "    print(\"#######################################\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    try:\n",
    "        # Crear RFE con el modelo seleccionado\n",
    "        rfe = RFE(model, n_features_to_select=10)  # Selecciona el número de características importantes\n",
    "        \n",
    "        # Crear el pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('feature_selection', rfe),    # Selección de características con RFE\n",
    "            ('classification', model)      # Modelo seleccionado\n",
    "        ])\n",
    "        \n",
    "        # Entrenar el pipeline con todo el dataset disponible\n",
    "        pipeline.fit(x_total, y_total)\n",
    "        \n",
    "        # Obtener las características seleccionadas por RFE\n",
    "        selected_features = rfe.support_\n",
    "        feature_ranking = rfe.ranking_\n",
    "        print(\"Características seleccionadas (True = seleccionada):\", selected_features)\n",
    "        print(\"Ranking de las características (1 = seleccionada):\", feature_ranking)\n",
    "        \n",
    "        print(f\"{modelName}, Done Training with all available data\")\n",
    "        \n",
    "        # Devolver el pipeline entrenado con RFE aplicado\n",
    "        return pipeline\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during final training: {e}\")\n",
    "        return None"
   ],
   "id": "a1b7faf5cf0f134b",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T07:53:17.316423Z",
     "start_time": "2024-09-11T07:53:17.304056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Entrenar el modelo final con los datos completos\n",
    "#final_model = RandomForestClassifier()\n",
    "#final_model.fit(x_total, y_total)  # Ajustar con todos los datos\n",
    "\n",
    "# Guardar el modelo en un archivo usando pickle\n",
    "#with open('final_model.pkl', 'wb') as model_file:\n",
    "#    pickle.dump(final_model, model_file)\n",
    "\n",
    "#print(\"Modelo guardado exitosamente.\")"
   ],
   "id": "fd99f2cf9ba35a2b",
   "outputs": [],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
